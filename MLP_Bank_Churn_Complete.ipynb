{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ====== 1. Generate Dataset ======\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'credit_score': np.random.normal(650, 100, n_samples).astype(int),\n",
    "    'balance': np.round(np.random.exponential(50000, n_samples), 2),\n",
    "    'products_number': np.random.randint(1, 5, n_samples),\n",
    "    'estimated_salary': np.round(np.random.uniform(20000, 150000, n_samples), 2),\n",
    "    'country': np.random.choice(['Germany', 'France', 'Spain'], n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "    'tenure': np.random.randint(0, 10, n_samples),\n",
    "    'has_credit_card': np.random.choice([0, 1], n_samples),\n",
    "    'is_active_member': np.random.choice([0, 1], n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create target variable\n",
    "df['churn'] = (\n",
    "    (df['balance'] > 80000) | \n",
    "    (df['credit_score'] < 550) |\n",
    "    (df['products_number'] == 1) |\n",
    "    ((df['age'] > 65) & (df['is_active_member'] == 0))\n",
    ").astype(int)\n",
    "\n",
    "print(f\"Dataset: {len(df)} rows Ã— {len(df.columns)} columns\")\n",
    "print(f\"Churn rate: {df['churn'].mean():.2%}\\n\")\n",
    "\n",
    "# ====== 2. Data Preprocessing ======\n",
    "# Encode categorical variables\n",
    "le_country = LabelEncoder()\n",
    "le_gender = LabelEncoder()\n",
    "df['country'] = le_country.fit_transform(df['country'])\n",
    "df['gender'] = le_gender.fit_transform(df['gender'])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('churn', axis=1).values\n",
    "y = df['churn'].values.reshape(-1, 1)\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\\n\")\n",
    "\n",
    "# ====== 3. Build 3-Layer MLP with NumPy ======\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Xavier initialization for better convergence\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0/input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0/hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Calculate gradient for output layer\n",
    "        dz2 = output - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Calculate gradient for hidden layer\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        loss = -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            loss = self.compute_loss(y, output)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if (epoch + 1) % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "\n",
    "# ====== 4. Train Model ======\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "mlp = MLP(input_size, hidden_size, output_size)\n",
    "print(\"Training MLP with improved settings...\\n\")\n",
    "losses = mlp.train(X_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# ====== 5. Evaluation ======\n",
    "y_pred_train = mlp.predict(X_train)\n",
    "y_pred_test = mlp.predict(X_test)\n",
    "\n",
    "train_accuracy = np.mean(y_pred_train == y_train)\n",
    "test_accuracy = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print(f\"\\nâœ“ Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"âœ“ Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====== Question 3: Compare Activation Functions ======\n",
    "\n",
    "class MLP_MultiActivation:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0/input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0/hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def tanh_derivative(self, z):\n",
    "        return 1 - z**2\n",
    "    \n",
    "    def activate(self, z):\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            return self.sigmoid(z)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return self.relu(z)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return self.tanh(z)\n",
    "    \n",
    "    def activate_derivative(self, z):\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            return self.sigmoid_derivative(z)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return self.relu_derivative(z)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return self.tanh_derivative(z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.activate(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)  # Output layer always sigmoid for binary classification\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dz2 = output - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradient\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.activate_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        loss = -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate, verbose=False):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            loss = self.compute_loss(y, output)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 200 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "\n",
    "# Train and compare different activation functions\n",
    "activations = ['sigmoid', 'relu', 'tanh']\n",
    "results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Comparing Activation Functions: Sigmoid vs ReLU vs Tanh\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"\\n--- Training with {activation.upper()} activation ---\")\n",
    "    \n",
    "    model = MLP_MultiActivation(input_size, hidden_size, output_size, activation=activation)\n",
    "    losses = model.train(X_train, y_train, epochs=1000, learning_rate=0.1, verbose=True)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    train_acc = np.mean(y_pred_train == y_train)\n",
    "    test_acc = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    results[activation] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'final_loss': losses[-1],\n",
    "        'losses': losses\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"âœ“ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Activation':<12} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for activation in activations:\n",
    "    r = results[activation]\n",
    "    print(f\"{activation.upper():<12} {r['train_accuracy']:<12.4f} {r['test_accuracy']:<12.4f} {r['final_loss']:<12.4f}\")\n",
    "\n",
    "# Find best activation\n",
    "best_activation = max(results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "print(f\"\\nðŸ† Best Activation: {best_activation[0].upper()} with Test Accuracy = {best_activation[1]['test_accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
