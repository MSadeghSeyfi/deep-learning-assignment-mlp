{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ====== 1. Generate Dataset ======\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'credit_score': np.random.normal(650, 100, n_samples).astype(int),\n",
    "    'balance': np.round(np.random.exponential(50000, n_samples), 2),\n",
    "    'products_number': np.random.randint(1, 5, n_samples),\n",
    "    'estimated_salary': np.round(np.random.uniform(20000, 150000, n_samples), 2),\n",
    "    'country': np.random.choice(['Germany', 'France', 'Spain'], n_samples),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "    'tenure': np.random.randint(0, 10, n_samples),\n",
    "    'has_credit_card': np.random.choice([0, 1], n_samples),\n",
    "    'is_active_member': np.random.choice([0, 1], n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create target variable\n",
    "df['churn'] = (\n",
    "    (df['balance'] > 80000) | \n",
    "    (df['credit_score'] < 550) |\n",
    "    (df['products_number'] == 1) |\n",
    "    ((df['age'] > 65) & (df['is_active_member'] == 0))\n",
    ").astype(int)\n",
    "\n",
    "print(f\"Dataset: {len(df)} rows Ã— {len(df.columns)} columns\")\n",
    "print(f\"Churn rate: {df['churn'].mean():.2%}\\n\")\n",
    "\n",
    "# ====== 2. Data Preprocessing ======\n",
    "# Encode categorical variables\n",
    "le_country = LabelEncoder()\n",
    "le_gender = LabelEncoder()\n",
    "df['country'] = le_country.fit_transform(df['country'])\n",
    "df['gender'] = le_gender.fit_transform(df['gender'])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('churn', axis=1).values\n",
    "y = df['churn'].values.reshape(-1, 1)\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\\n\")\n",
    "\n",
    "# ====== 3. Build 3-Layer MLP with NumPy ======\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Xavier initialization for better convergence\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0/input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0/hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Calculate gradient for output layer\n",
    "        dz2 = output - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Calculate gradient for hidden layer\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        loss = -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            loss = self.compute_loss(y, output)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if (epoch + 1) % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "\n",
    "# ====== 4. Train Model ======\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "mlp = MLP(input_size, hidden_size, output_size)\n",
    "print(\"Training MLP with improved settings...\\n\")\n",
    "losses = mlp.train(X_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# ====== 5. Evaluation ======\n",
    "y_pred_train = mlp.predict(X_train)\n",
    "y_pred_test = mlp.predict(X_test)\n",
    "\n",
    "train_accuracy = np.mean(y_pred_train == y_train)\n",
    "test_accuracy = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print(f\"\\nâœ“ Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"âœ“ Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====== Question 3: Compare Activation Functions ======\n",
    "\n",
    "class MLP_MultiActivation:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0/input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0/hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def tanh_derivative(self, z):\n",
    "        return 1 - z**2\n",
    "    \n",
    "    def activate(self, z):\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            return self.sigmoid(z)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return self.relu(z)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return self.tanh(z)\n",
    "    \n",
    "    def activate_derivative(self, z):\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            return self.sigmoid_derivative(z)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return self.relu_derivative(z)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return self.tanh_derivative(z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.activate(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)  # Output layer always sigmoid for binary classification\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dz2 = output - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradient\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.activate_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        loss = -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate, verbose=False):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output, learning_rate)\n",
    "            loss = self.compute_loss(y, output)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 200 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "\n",
    "# Train and compare different activation functions\n",
    "activations = ['sigmoid', 'relu', 'tanh']\n",
    "results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Comparing Activation Functions: Sigmoid vs ReLU vs Tanh\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"\\n--- Training with {activation.upper()} activation ---\")\n",
    "    \n",
    "    model = MLP_MultiActivation(input_size, hidden_size, output_size, activation=activation)\n",
    "    losses = model.train(X_train, y_train, epochs=1000, learning_rate=0.1, verbose=True)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    train_acc = np.mean(y_pred_train == y_train)\n",
    "    test_acc = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    results[activation] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'final_loss': losses[-1],\n",
    "        'losses': losses\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"âœ“ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Activation':<12} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for activation in activations:\n",
    "    r = results[activation]\n",
    "    print(f\"{activation.upper():<12} {r['train_accuracy']:<12.4f} {r['test_accuracy']:<12.4f} {r['final_loss']:<12.4f}\")\n",
    "\n",
    "# Find best activation\n",
    "best_activation = max(results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "print(f\"\\nðŸ† Best Activation: {best_activation[0].upper()} with Test Accuracy = {best_activation[1]['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====== Question 4: Experiment with Learning Rates ======\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "lr_results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Experimenting with Different Learning Rates\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n--- Training with Learning Rate = {lr} ---\")\n",
    "    \n",
    "    model = MLP(input_size, hidden_size, output_size)\n",
    "    losses = model.train(X_train, y_train, epochs=1000, learning_rate=lr)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    train_acc = np.mean(y_pred_train == y_train)\n",
    "    test_acc = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    lr_results[lr] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'losses': losses\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"âœ“ Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"âœ“ Final Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# Visualize loss curves\n",
    "print(\"\\n--- Visualizing Convergence Speed ---\")\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(lr_results[lr]['losses'], label=f'LR = {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves for Different Learning Rates')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "lrs = [str(lr) for lr in learning_rates]\n",
    "train_accs = [lr_results[lr]['train_accuracy'] for lr in learning_rates]\n",
    "test_accs = [lr_results[lr]['test_accuracy'] for lr in learning_rates]\n",
    "\n",
    "x = np.arange(len(lrs))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, train_accs, width, label='Train Accuracy', alpha=0.8)\n",
    "plt.bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xticks(x, lrs)\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LEARNING RATE COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'LR':<10} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for lr in learning_rates:\n",
    "    r = lr_results[lr]\n",
    "    print(f\"{lr:<10} {r['train_accuracy']:<12.4f} {r['test_accuracy']:<12.4f} {r['losses'][-1]:<12.4f}\")\n",
    "\n",
    "# Find best learning rate\n",
    "best_lr = max(lr_results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "print(f\"\\nðŸ† Best Learning Rate: {best_lr[0]} with Test Accuracy = {best_lr[1]['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====== Question 5: Analyze Hidden Layer Size Impact ======\n",
    "\n",
    "# Test different hidden layer sizes\n",
    "hidden_sizes = [16, 64, 128]\n",
    "hidden_results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Analyzing Hidden Layer Size Impact\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for h_size in hidden_sizes:\n",
    "    print(f\"\\n--- Training with Hidden Size = {h_size} neurons ---\")\n",
    "    \n",
    "    model = MLP(input_size, h_size, output_size)\n",
    "    losses = model.train(X_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    train_acc = np.mean(y_pred_train == y_train)\n",
    "    test_acc = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = (input_size * h_size + h_size) + (h_size * output_size + output_size)\n",
    "    \n",
    "    hidden_results[h_size] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'losses': losses,\n",
    "        'parameters': total_params\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"âœ“ Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"âœ“ Total Parameters: {total_params}\")\n",
    "\n",
    "# Visualize results\n",
    "print(\"\\n--- Visualizing Hidden Layer Impact ---\")\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "for h_size in hidden_sizes:\n",
    "    plt.plot(hidden_results[h_size]['losses'], label=f'{h_size} neurons')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves for Different Hidden Sizes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "sizes = [str(h) for h in hidden_sizes]\n",
    "train_accs = [hidden_results[h]['train_accuracy'] for h in hidden_sizes]\n",
    "test_accs = [hidden_results[h]['test_accuracy'] for h in hidden_sizes]\n",
    "\n",
    "x = np.arange(len(sizes))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, train_accs, width, label='Train Accuracy', alpha=0.8)\n",
    "plt.bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xticks(x, sizes)\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Parameters vs Accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "params = [hidden_results[h]['parameters'] for h in hidden_sizes]\n",
    "test_accs = [hidden_results[h]['test_accuracy'] for h in hidden_sizes]\n",
    "\n",
    "plt.plot(params, test_accs, marker='o', linewidth=2, markersize=10)\n",
    "for i, h in enumerate(hidden_sizes):\n",
    "    plt.annotate(f'{h} neurons', (params[i], test_accs[i]), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Complexity vs Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HIDDEN LAYER SIZE COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Hidden Size':<15} {'Parameters':<15} {'Train Acc':<12} {'Test Acc':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for h_size in hidden_sizes:\n",
    "    r = hidden_results[h_size]\n",
    "    print(f\"{h_size:<15} {r['parameters']:<15} {r['train_accuracy']:<12.4f} {r['test_accuracy']:<12.4f}\")\n",
    "\n",
    "# Find best hidden size\n",
    "best_hidden = max(hidden_results.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "print(f\"\\nðŸ† Best Hidden Size: {best_hidden[0]} neurons with Test Accuracy = {best_hidden[1]['test_accuracy']:.4f}\")\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "overfitting_gap = []\n",
    "for h_size in hidden_sizes:\n",
    "    gap = hidden_results[h_size]['train_accuracy'] - hidden_results[h_size]['test_accuracy']\n",
    "    overfitting_gap.append(gap)\n",
    "    status = \"Possible Overfitting\" if gap > 0.05 else \"Good Generalization\"\n",
    "    print(f\"Hidden Size {h_size}: Train-Test Gap = {gap:.4f} â†’ {status}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
